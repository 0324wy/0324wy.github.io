<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog | Yan Wang</title>
    <link>https://0324wy.github.io/blog/</link>
      <atom:link href="https://0324wy.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <description>Blog</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 29 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://0324wy.github.io/media/icon_hua5a76ffd5cf5a7c3caab46aff2309b2f_3105_512x512_fill_lanczos_center_3.png</url>
      <title>Blog</title>
      <link>https://0324wy.github.io/blog/</link>
    </image>
    
    <item>
      <title>TensorIR, RelayIR, and IRModule in TVM</title>
      <link>https://0324wy.github.io/blog/blog-4-tvmconcept/</link>
      <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/blog/blog-4-tvmconcept/</guid>
      <description>&lt;h2 id=&#34;concept-of-tensorir-relayir-irmodule-prim_fun-tvm-script&#34;&gt;Concept of TensorIR, RelayIR, IRModule, Prim_fun, TVM Script&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Two levels represent the entire neural network program; the first level is a computational graph, and the second level is the tensor program. The previous one is called RelayIR, and the second one is called TensorIR. What we use in TVM to represent the TensorIR is called TVM Script.&lt;/li&gt;
&lt;li&gt;The tensor program contains the input and output, nested loop, and computation statement, which is a more fine-grained representation of the entire neural network program.&lt;/li&gt;
&lt;li&gt;The difference between the neural network program and the tensor program is the latter is framework-independent. It can represent the neural network program from PyTorch, TensorFlow, and others in a unified format.&lt;/li&gt;
&lt;li&gt;The above demonstration is from the vertical perspective.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://pic1.zhimg.com/70/v2-dcb979b6ad713fb751c11e1fb58b779b_1440w.image?source=172ae18b&amp;amp;biz_tag=Post&#34; alt=&#34;TVM 自底向上（一）：基本框架和概念&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;From the horizontal perspective, each graph contains multiple computation nodes(such as add, mm, ReLu, etc). Each of these nodes has its corresponding part in TensorIR; Each node corresponds to a prim_fun in TensorIR.&lt;/li&gt;
&lt;li&gt;Part of the graph contains one or more nodes, which is called a subgraph. Each subgraph corresponds to the combination of several prim_fun in TensorIR. The subgraph in RelayIR and the combination of several prim_fun in TensorIR are both called IRModule. Which is a container that includes several nodes or prim_func.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;four-ways-to-get-tensorir&#34;&gt;Four ways to get TensorIR&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Write TVM Script&lt;/li&gt;
&lt;li&gt;TE Expression&lt;/li&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;li&gt;Load from PyTorch&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;schedule-compute-decomposition&#34;&gt;Schedule Compute Decomposition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;source-code&#34;&gt;Source Code&lt;/h2&gt;
&lt;h3 id=&#34;object&#34;&gt;Object&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;All the IR datatypes should be inherent in the base class called Object, which defines the attributes of this class, and a function called VisitAttrs.&lt;/li&gt;
&lt;li&gt;In this way, the IR datatypes are in a unified format.&lt;/li&gt;
&lt;li&gt;Serialize/format/reflection/python bind/hash, to put it simply, interact with these IRs.&lt;/li&gt;
&lt;li&gt;The visitor pattern is a design pattern that makes the data and operations of that data separate. The benefits are from two aspects: 1. When we add new operations or change the original operations, we don&amp;rsquo;t have to change the code of data. 2. VisitAttrs allows us can process all the fields in a unified way.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;TensorNode&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Object&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;Array&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Expr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;Type&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;source&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;operation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;can&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;be&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;Operation&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;op&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;source&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;operation&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;constructor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;TensorNode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;VisitAttrs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;AttrVisitor&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;final&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Visit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;shape&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Visit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dtype&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Visit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;op&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;op&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Visit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;value_index&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;runtime&#34;&gt;Runtime&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is runtime? After we compile the code or the model code, we still need some other support to run the code, such as memory management, handling errors, etc. So, we develop a program to do that, and that program is called runtime.&lt;/li&gt;
&lt;li&gt;In the following example, you need to: 1. load the compiled model code; 2. initialize new data; 3. search the needed function; 4. run the function and print the result.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tvm&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mod&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tvm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;runtime&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;compiled_artifact.so&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# tvm.runtime.Module&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;arr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tvm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tvm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# tvm.runtime.NDArray&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;fun&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mod&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;addone&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# tvm.runtime.PackedFunc&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;fun&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;numpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;shared_ptr&#34;&gt;shared_ptr&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;include/tvm/runtime/object.h defines Object and ObjectRef as two types.&lt;/li&gt;
&lt;li&gt;The ObjectRef can be seen as the share_ptr&amp;lt;Object)&lt;/li&gt;
&lt;li&gt;The shared_ptr has three features
&lt;ul&gt;
&lt;li&gt;Automatic management of memory, specifically, automatically deletes the Object if it is released.&lt;/li&gt;
&lt;li&gt;One Object could have multiple shared_ptr&lt;/li&gt;
&lt;li&gt;Reference counter: if the count of references is 0, the object will be deleted.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#include &amp;lt;iostream&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#include &amp;lt;memory&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shared_ptr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_shared&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Creates&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shared_ptr&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;an&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shared_ptr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Now&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;share&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ownership&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;same&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Use count: &amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use_count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;share&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ownership&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;releases&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ownership&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Use count after reset: &amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use_count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;only&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;owns&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;now&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;packedfunc&#34;&gt;PackedFunc&lt;/h3&gt;
&lt;p&gt;==TODO==&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TVM has a Foreign Function Interface mechanism, which allows any language to call a function written by any language.&lt;/li&gt;
&lt;li&gt;What should we do for the function? First of all, we need to erase the parameter type and return type of the function.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;tvm/runtime/packed_func.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;MyAdd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TVMArgs&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TVMRetValue&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// Access arguments with type conversion
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// Set the return value
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;CallPacked&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;PackedFunc&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;myadd&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PackedFunc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MyAdd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// Wrap MyAdd as a PackedFunc
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;myadd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// Call PackedFunc, returns 3
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;module&#34;&gt;Module&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The result of the compilation is Runtime.Module, which is a hashmap&amp;lt;functionname, PackedFunc&amp;gt;.&lt;/li&gt;
&lt;li&gt;When this hashmap is retrieved, the caller gets the function and sets up the function and then runs the function.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Memory and Computation Cost of GPT</title>
      <link>https://0324wy.github.io/blog/blog-3-gptcost/</link>
      <pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/blog/blog-3-gptcost/</guid>
      <description>&lt;h2 id=&#34;memory&#34;&gt;Memory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Activation values: 50 times the parameter memory&lt;/li&gt;
&lt;li&gt;Optimizer state: 2 times the parameter memory&lt;/li&gt;
&lt;li&gt;Gradients: 1 times the parameter memory&lt;/li&gt;
&lt;li&gt;KV Cache: 0.5 times the memory&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;number-of-parameters&#34;&gt;Number of Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Number of parameters: approximately (number)：$12h^2l$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;memory-usage&#34;&gt;Memory Usage&lt;/h2&gt;
&lt;p&gt;Independent of input data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Memory usage during training: Excluding activation values&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parameters: float16 + float32：$\Phi$
&lt;ul&gt;
&lt;li&gt;Gradients: float16 + float32：$\Phi$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parameters for AdamW: float32：$2\Phi$&lt;/li&gt;
&lt;li&gt;Total：$20\Phi Bytes$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory usage during inference: Excluding activation values&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Parameters: float16：$\Phi$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Total: $2\Phi Bytes$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activation-values&#34;&gt;Activation Values&lt;/h2&gt;
&lt;p&gt;Approximately: per unit (Bytes)：$(34bsh + 5bs^2a)l$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When b=1, approximately 0.8 times the parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kv-cache&#34;&gt;KV Cache&lt;/h2&gt;
&lt;p&gt;Approximately: per unit (number)：$2b(s + n)l$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approximately 0.5 times the parameter memory&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computation&#34;&gt;Computation&lt;/h2&gt;
&lt;h2 id=&#34;computational-complexity&#34;&gt;Computational Complexity&lt;/h2&gt;
&lt;p&gt;Approximately: per unit (count)：$24bsh^2l$&lt;/p&gt;
&lt;h2 id=&#34;relationship-between-computational-complexity-and-number-of-parameters&#34;&gt;Relationship between Computational Complexity and Number of Parameters&lt;/h2&gt;
&lt;p&gt;Approximately: 2 times the number of tokens * number of parameters&lt;/p&gt;
&lt;p&gt;$$
\frac{24bsh^2l} {12h^2l * bs} = 2
$$&lt;/p&gt;
&lt;h2 id=&#34;computation-time&#34;&gt;Computation Time&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Backward propagation is approximately twice the computation of forward propagation.&lt;/li&gt;
&lt;li&gt;Activation value recomputation is approximately the same as forward propagation.&lt;/li&gt;
&lt;li&gt;Total is 8 times.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\frac{8 * number of tokens * number of parameters} {number of GPUs * peak flops per GPU * GPU utilization}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPU utilization is approximately between 0.3 and 0.55.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Good Open Courses for Computer Science Learning</title>
      <link>https://0324wy.github.io/blog/blog-2-csmaterial/</link>
      <pubDate>Mon, 13 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/blog/blog-2-csmaterial/</guid>
      <description>&lt;h2 id=&#34;introduction-to-computer-science&#34;&gt;Introduction to Computer Science&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MIT 6.0001&lt;/li&gt;
&lt;li&gt;Harvard CS50&lt;/li&gt;
&lt;li&gt;Berkeley CS61A&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-structures-and-algorithms&#34;&gt;Data Structures and Algorithms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stanford CS106&lt;/li&gt;
&lt;li&gt;MIT 6.006, 6.046&lt;/li&gt;
&lt;li&gt;Berkeley CS61A, CS61B&lt;/li&gt;
&lt;li&gt;Princeton&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;operating-systems&#34;&gt;Operating Systems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CMU 15-213&lt;/li&gt;
&lt;li&gt;Berkeley CS162, CS262&lt;/li&gt;
&lt;li&gt;MIT 6.828&lt;/li&gt;
&lt;li&gt;6.S081&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computer-organization-and-architecture&#34;&gt;Computer Organization and Architecture&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MIT 6.004&lt;/li&gt;
&lt;li&gt;Berkeley CS61C&lt;/li&gt;
&lt;li&gt;MHRD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computer-networks&#34;&gt;Computer Networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stanford CS144&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;databases&#34;&gt;Databases&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CMU 15-445&lt;/li&gt;
&lt;li&gt;MIT 6.824, 6.830&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;software-engineering&#34;&gt;Software Engineering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MIT 6.031&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;distributed-systems&#34;&gt;Distributed Systems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MIT 6.824&lt;/li&gt;
&lt;li&gt;MIT 6.033
&lt;ul&gt;
&lt;li&gt;Principles of Computer System Design: An Introduction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;compiler-design&#34;&gt;Compiler Design&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stanford CS143&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;parallel-computing&#34;&gt;Parallel Computing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CS 149&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computer-graphics&#34;&gt;Computer Graphics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Games101&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computer-system-security&#34;&gt;Computer System Security&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;6.858&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;projects&#34;&gt;Projects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Operating System: 6.828&lt;/li&gt;
&lt;li&gt;Distributed Systems: 6.824&lt;/li&gt;
&lt;li&gt;Relational Database: CMU 15-445&lt;/li&gt;
&lt;li&gt;Parallel Computing: CS 149&lt;/li&gt;
&lt;li&gt;Compiler Design: Stanford CS143&lt;/li&gt;
&lt;li&gt;Computer Graphics: Games101&lt;/li&gt;
&lt;li&gt;Computer Networks: Stanford CS144&lt;/li&gt;
&lt;li&gt;Computer System Security: 6.858&lt;/li&gt;
&lt;li&gt;CSAPP&lt;/li&gt;
&lt;li&gt;Muduo: Web Server&lt;/li&gt;
&lt;li&gt;TinSTL: STL&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Tiger Book&amp;rdquo;: Compiler&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CS DIY: &lt;a href=&#34;https://csdiy.wiki/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS DIY Wiki&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Experience of Self-learning Driven by Interest</title>
      <link>https://0324wy.github.io/blog/blog-1-experience/</link>
      <pubDate>Mon, 13 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/blog/blog-1-experience/</guid>
      <description>&lt;p style=&#34;text-align: justify;&#34;&gt;Unlike my peers majoring in computer science, I pursued a self-learning path based on my interests. While encountering some difficulties, the journey was also highly rewarding.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;My first exposure to computer science was during my freshman year, where I learned C language - my first programming language. I found it fascinating and read the entire textbook thoroughly. As a result, I performed well in the course and scored 95 points. In my sophomore and junior years, I participated in mathematical modeling competitions and was introduced to neural networks. Although I didn&amp;rsquo;t fully understand them at the time, it sparked my curiosity to learn more. In my senior year, I heard about concepts like deep learning and artificial intelligence, and realized that they were built on neural networks, which I had not yet fully grasped. Driven by my curiosity, I planned to systematically learn the principles of these topics.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Throughout my graduate studies, starting from my senior year, I continued to explore computer science knowledge driven by my passion. I delved into Python, Machine Learning, Deep Learning, Java, C++, Data Structures and Algorithms. Along the way, I was introduced to lower-level concepts, so I started learning Operating Systems, Computer Networks, Database Principles, and Design Patterns. I also pursued many mathematics courses, including Matrix Theory, Partial Differential Equation, and Computational Methods, with the aim of becoming a well-rounded engineering student with a strong foundation in mathematics and programming skills.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;During my master&amp;rsquo;s degree, I chose to research on the intersection of deep learning and civil engineering. In a situation where the research group had no accumulation in this area, I had to figure things out on my own. I aspired to apply for a Ph.D. in computer science during my graduate studies, but struggled to find a clear path. I eventually decided to work instead. However, as I gained more exposure to information, I discovered that a path to a Ph.D. could be pursued through internships at research labs to gain research experience and skills.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;After graduation, I worked as a web backend developer at Baidu. As the entire backend system was a distributed system, I began learning about distributed systems and studied various open-source project architectures, such as Redis and Zookeeper. I also read papers on machine learning, including the Bert and GPT series.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Looking back on my entire experience of self-learning, I&amp;rsquo;ve realized that I&amp;rsquo;ve gained not only knowledge, but also the ability to select appropriate learning materials, effectively manage study time and energy, and develop the courage to overcome difficulties. With these skills, I am excited to continue exploring new areas of knowledge and challenging myself further.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;My interest in both distributed systems and machine learning is the reason I want to work in MLSys. I believe that the infrastructure for large models is critical and promising. Additionally, this field is closer to industry and practical applications compared to other fields. The results of this field are more easily deployed in the development process and products, rather than just remaining on paper. This aligns with my research goals, which aim to have a positive impact on subsequent research in this field and its applications in industry.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
