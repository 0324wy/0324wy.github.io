
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":" My name is Yan Wang (“王岩” in Chinese). I am an incoming PhD student at Northeastern University, advised by Prof. Xiaolin Xu.\nMy research interests lie in efficient machine learning, from both algorithmic and system/architecture perspectives. Recently, I have been working on accelerating the inference and training of large models.\n","date":1686182400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1686182400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"My name is Yan Wang (“王岩” in Chinese). I am an incoming PhD student at Northeastern University, advised by Prof. Xiaolin Xu.\nMy research interests lie in efficient machine learning, from both algorithmic and system/architecture perspectives.","tags":null,"title":"Yan Wang","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://0324wy.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Yan Wang"],"categories":null,"content":"Memory Activation values: 50 times the parameter memory Optimizer state: 2 times the parameter memory Gradients: 1 times the parameter memory KV Cache: 0.5 times the memory Number of Parameters Number of parameters: approximately (number)：$12h^2l$ Memory Usage Independent of input data\nMemory usage during training: Excluding activation values\nParameters: float16 + float32：$\\Phi$ Gradients: float16 + float32：$\\Phi$ Parameters for AdamW: float32：$2\\Phi$ Total：$20\\Phi Bytes$ Memory usage during inference: Excluding activation values\nParameters: float16：$\\Phi$\nTotal: $2\\Phi Bytes$\nActivation Values Approximately: per unit (Bytes)：$(34bsh + 5bs^2a)l$\nWhen b=1, approximately 0.8 times the parameters KV Cache Approximately: per unit (number)：$2b(s + n)l$\nApproximately 0.5 times the parameter memory Computation Computational Complexity Approximately: per unit (count)：$24bsh^2l$\nRelationship between Computational Complexity and Number of Parameters Approximately: 2 times the number of tokens * number of parameters\n$$ \\frac{24bsh^2l} {12h^2l * bs} = 2 $$\nComputation Time Backward propagation is approximately twice the computation of forward propagation. Activation value recomputation is approximately the same as forward propagation. Total is 8 times. $$ \\frac{8 * number of tokens * number of parameters} {number of GPUs * peak flops per GPU * GPU utilization} $$\nGPU utilization is approximately between 0.3 and 0.55. ","date":1686182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686182400,"objectID":"aa77f3cf077c058051e9133d7729fe06","permalink":"https://0324wy.github.io/blog/blog-3-gptcost/","publishdate":"2023-06-08T00:00:00Z","relpermalink":"/blog/blog-3-gptcost/","section":"blog","summary":"Memory Activation values: 50 times the parameter memory Optimizer state: 2 times the parameter memory Gradients: 1 times the parameter memory KV Cache: 0.5 times the memory Number of Parameters Number of parameters: approximately (number)：$12h^2l$ Memory Usage Independent of input data","tags":null,"title":"The Memory and Computation Cost of GPT","type":"blog"},{"authors":null,"categories":null,"content":"Completed an online course on deep learning systems offered by CMU in order to delve into the internals of PyTorch and TensorFlow, and understand how they function at a fundamental level.\nDesigned and built a deep learning library called Needle, capable of efficient GPU-based operations, automatic differentiation of all implemented functions, and the necessary modules to support parameterized layers, loss functions, data loaders, and optimizers.\nProject 0 Build a basic softmax regression algorithm, plus a simple two-layer neural network. Create these implementations both in native Python (using the numpy library), and (for softmax regression) in native C/C++.\n✅ A basic add function\n✅ Loading MNIST data: parse_mnist function\n✅Softmax loss: softmax_loss function\n✅Stochastic gradient descent for softmax regression\n✅SGD for a two-layer neural network\n✅Softmax regression in C++\nProject 1 Build a basic automatic differentiation framework, then use this to re-implement the simple two-layer neural network we used for the MNIST digit classification problem in HW0.\n✅Implementing forward computation\n✅PowerScalar ✅EWiseDiv ✅DivScalar ✅MatMul ✅Summation ✅BroadcastTo ✅Reshape ✅Negate ✅Transpose ✅Implementing backward computation\n✅EWiseDiv ✅DivScalar ✅MatMul ✅Summation ✅BroadcastTo ✅Reshape ✅Negate ✅Transpose ✅Topological sort: allow us to traverse through (forward or backward) the computation graph, computing gradients along the way\n✅Implementing reverse mode differentiation\n✅Softmax loss\n✅SGD for a two-layer neural network\nProject 2 Implement a neural network library in the needle framework.\n✅Implement a few different methods for weight initialization\n✅Implement additional modules\n✅Linear: needle.nn.Linear class ✅ReLU:needle.nn.ReLU class ✅Sequential: needle.nn.Sequential class ✅LogSumExp: needle.ops.LogSumExp class ✅SoftmaxLoss: needle.nn.SoftmaxLoss class ✅LayerNorm1d: needle.nn.LayerNorm1d class ✅Flatten: needle.nn.Flatten class ✅BatchNorm1d: needle.nn.BatchNorm1d class ✅Dropout: needle.nn.Dropout class ✅Residual: needle.nn.Residual class ✅Implement the step function of the following optimizers.\n✅SGD: needle.optim.SGD class ✅Adam: needle.optim.Adam class ✅Implement two data primitives: needle.data.DataLoader and needle.data.Dataset\n✅Transformations: RandomFlipHorizontal function and RandomFlipHorizontal class ✅Dataset: needle.data.MNISTDataset class ✅Dataloader: needle.data.Dataloader class ✅Build and train an MLP ResNet\n✅ResidualBlock: ResidualBlock function ✅MLPResNet: MLPResNet function ✅Epoch: epoch function ✅Train Mnist: train_mnist function Project 3 Build a simple backing library for the processing that underlies most deep learning systems: the n-dimensional array (a.k.a. the NDArray).\n✅Python array operations\n✅reshape: reshape function ✅permute: permute function ","date":1679443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679443200,"objectID":"385bd4c6848c82c2efb0e22fb1510449","permalink":"https://0324wy.github.io/projects/needle/","publishdate":"2023-03-22T00:00:00Z","relpermalink":"/projects/needle/","section":"projects","summary":"Designed and built a deep learning library called Needle, comparable to a very minimal version of PyTorch or TensorFlow, capable of efficient GPU-based operations, automatic differentiation of all implemented functions, and the necessary modules to support parameterized layers, loss functions, data loaders, and optimizers.","tags":["Machine Learning Systems"],"title":"Designed and Built a Deep Learning Library Called Needle","type":"projects"},{"authors":["Yan Wang"],"categories":null,"content":"Introduction to Computer Science MIT 6.0001 Harvard CS50 Berkeley CS61A Data Structures and Algorithms Stanford CS106 MIT 6.006, 6.046 Berkeley CS61A, CS61B Princeton Operating Systems CMU 15-213 Berkeley CS162, CS262 MIT 6.828 6.S081 Computer Organization and Architecture MIT 6.004 Berkeley CS61C MHRD Computer Networks Stanford CS144 Databases CMU 15-445 MIT 6.824, 6.830 Software Engineering MIT 6.031 Distributed Systems MIT 6.824 MIT 6.033 Principles of Computer System Design: An Introduction Compiler Design Stanford CS143 Parallel Computing CS 149 Computer Graphics Games101 Computer System Security 6.858 Projects Operating System: 6.828 Distributed Systems: 6.824 Relational Database: CMU 15-445 Parallel Computing: CS 149 Compiler Design: Stanford CS143 Computer Graphics: Games101 Computer Networks: Stanford CS144 Computer System Security: 6.858 CSAPP Muduo: Web Server TinSTL: STL “Tiger Book”: Compiler CS DIY: CS DIY Wiki\n","date":1678665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680134400,"objectID":"c8efabee56aa60bcb50e90460d48df76","permalink":"https://0324wy.github.io/blog/blog-2-csmaterial/","publishdate":"2023-03-13T00:00:00Z","relpermalink":"/blog/blog-2-csmaterial/","section":"blog","summary":"Introduction to Computer Science MIT 6.0001 Harvard CS50 Berkeley CS61A Data Structures and Algorithms Stanford CS106 MIT 6.006, 6.046 Berkeley CS61A, CS61B Princeton Operating Systems CMU 15-213 Berkeley CS162, CS262 MIT 6.","tags":null,"title":"Good Open Courses for Computer Science Learning","type":"blog"},{"authors":["Yan Wang"],"categories":null,"content":"Unlike my peers majoring in computer science, I pursued a self-learning path based on my interests. While encountering some difficulties, the journey was also highly rewarding.\nMy first exposure to computer science was during my freshman year, where I learned C language - my first programming language. I found it fascinating and read the entire textbook thoroughly. As a result, I performed well in the course and scored 95 points. In my sophomore and junior years, I participated in mathematical modeling competitions and was introduced to neural networks. Although I didn’t fully understand them at the time, it sparked my curiosity to learn more. In my senior year, I heard about concepts like deep learning and artificial intelligence, and realized that they were built on neural networks, which I had not yet fully grasped. Driven by my curiosity, I planned to systematically learn the principles of these topics.\nThroughout my graduate studies, starting from my senior year, I continued to explore computer science knowledge driven by my passion. I delved into Python, Machine Learning, Deep Learning, Java, C++, Data Structures and Algorithms. Along the way, I was introduced to lower-level concepts, so I started learning Operating Systems, Computer Networks, Database Principles, and Design Patterns. I also pursued many mathematics courses, including Matrix Theory, Partial Differential Equation, and Computational Methods, with the aim of becoming a well-rounded engineering student with a strong foundation in mathematics and programming skills.\nDuring my master’s degree, I chose to research on the intersection of deep learning and civil engineering. In a situation where the research group had no accumulation in this area, I had to figure things out on my own. I aspired to apply for a Ph.D. in computer science during my graduate studies, but struggled to find a clear path. I eventually decided to work instead. However, as I gained more exposure to information, I discovered that a path to a Ph.D. could be pursued through internships at research labs to gain research experience and skills.\nAfter graduation, I worked as a web backend developer at Baidu. As the entire backend system was a distributed system, I began learning about distributed systems and studied various open-source project architectures, such as Redis and Zookeeper. I also read papers on machine learning, including the Bert and GPT series.\nLooking back on my entire experience of self-learning, I’ve realized that I’ve gained not only knowledge, but also the ability to select appropriate learning materials, effectively manage study time and energy, and develop the courage to overcome difficulties. With these skills, I am excited to continue exploring new areas of knowledge and challenging myself further.\nMy interest in both distributed systems and machine learning is the reason I want to work in MLSys. I believe that the infrastructure for large models is critical and promising. Additionally, this field is closer to industry and practical applications compared to other fields. The results of this field are more easily deployed in the development process and products, rather than just remaining on paper. This aligns with my research goals, which aim to have a positive impact on subsequent research in this field and its applications in industry.\n","date":1678665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680134400,"objectID":"72af380c1d4ba165320a844030c9db00","permalink":"https://0324wy.github.io/blog/blog-1-experience/","publishdate":"2023-03-13T00:00:00Z","relpermalink":"/blog/blog-1-experience/","section":"blog","summary":"Unlike my peers majoring in computer science, I pursued a self-learning path based on my interests. While encountering some difficulties, the journey was also highly rewarding.\nMy first exposure to computer science was during my freshman year, where I learned C language - my first programming language.","tags":null,"title":"The Experience of Self-learning Driven by Interest","type":"blog"},{"authors":null,"categories":null,"content":"Baidu E-commerce Middle Platform Overview This project is the Baidu E-commerce Middle Platform project. The role of the E-commerce Middle Platform is to provide common e-commerce capabilities for Front Platform business parties, such as ordering, after-sales, promotions, and product management.\nIt is divided into four small teams, including Shop Center, Commodity Center, Marketing Center, and Trading Center. The Shop Center is mainly used to manage the stores of B-side merchants. The Commodity Center is used to manage the inventory and qualifications of products. The Trading Center is used to provide services such as ordering, shopping cart ordering, after-sales, and logistics inquiry. The Marketing Center is used to provide services such as creating coupons for merchants and calculating marketing prices.\nFront Platform business parties include products from different industries such as education, e-commerce, medical beauty, and life services. For example, AiQiCha is a product for the enterprise service industry, and ZhiLiaoHaoXue is a product for the education industry. Users enter the corresponding Front Platform products by clicking related links through the Baidu search engine. When users have e-commerce-related needs, they will use the services of the Middle Platform. For example, using the education industry product ZhiLiaoHaoXue to purchase an online course.\nDesign and Architecture Taking the example of the Front Platform application DuXiaoDian calling the Trading Center, there are a total of seven modules in the entire chain, each of which is independently deployed and uses a cluster architecture. The CDN servers, reverse proxy servers, and load balancing servers sit at the top of the chain. The Trading Center connects to both distributed database servers and distributed cache servers.\nCDN Servers\nCDN servers are designed to improve the availability and performance of a service by distributing it geographically closer to end-users. Essentially, a CDN is a cache server located closer to the users to speed up their access.\nReverse Proxy Servers\nReverse proxy servers serve a similar function to CDN servers, but they are located on the side of the website’s data center. Additionally, they are responsible for ensuring security and acting as a barrier between the web server and external traffic.\nLoading Balancing Servers\nLoad balancing servers are positioned in front of your backend servers and are responsible for distributing client requests across a group of servers. This is done in a way that maximizes speed and capacity utilization, while also ensuring that no single server becomes overloaded and negatively impacts performance. In the event that a server goes down, the load balancer redirects traffic to the remaining online servers.\nDistributed Cache Servers\nDistributed cache servers are servers that store webpages or other internet content locally. By storing frequently accessed data in temporary storage, cache servers both speed up access to data and reduce the demand on an enterprise’s bandwidth.\nIn this project, we are using Redis cluster as our distributed cache servers. Redis is known for its high-performance data read and write functions, and is widely used in caching scenarios. By using Redis, we can improve the performance of our business systems and better withstand high levels of concurrent traffic requests to the database.\nCore Question How To Solve the Data Consistency Issues Between Redis And MySQL?\nThis is also a classic problem in distributed systems - how to ensure data consistency between two heterogeneous systems.\nSetting up Redis cache is to improve the performance of the system by copying hot data from MySQL to Redis. When a read request is received, it is directly read from Redis instead of from MySQL database. If the data is not available in the Redis cache, it is read from the database, and the data is then copied from the database to Redis.\nHowever, this can cause a problem of data inconsistency between the two systems. For example, if the data X = 1 in MySQL is modified to X = 2, the old data X = 1 will still exist in the Redis cache.\nIf strong consistency is guaranteed, availability will be sacrificed. When modifying MySQL data, Redis must be updated synchronously, and no other read requests can be accepted during the update.\nTo ensure availability, only eventual consistency can be guaranteed. In this project, a solution to ensure eventual consistency is adopted. When the data X = 1 in MySQL is modified to X = 2, MySQL generates a Binlog record of the data change, and when the Binlog is listened to, the change record is added to the message queue. The consumer side consumes this message by deleting the corresponding data from Redis. When a new request comes in, if the data is not in the Redis cache, it is read from the database and copied to Redis. This ensures eventual consistency between Redis and MySQL data.\nDeleting the data X = 1 from Redis instead of modifying it to X …","date":1674813600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674813600,"objectID":"06c5830ea96bd860d516f5440e1bd44f","permalink":"https://0324wy.github.io/projects/baidu_e_commerce/","publishdate":"2023-01-27T10:00:00Z","relpermalink":"/projects/baidu_e_commerce/","section":"projects","summary":"Participated in the design and development of the Baidu e-commerce distributed order system, which provides general capabilities such as ordering, payment, fulfillment, after-sales, and evaluation. Took charge of functional iteration, defect modification, code maintenance.Contributed over 20,000 lines of code.","tags":["Systems Design and Development"],"title":"Baidu E-commerce Distributed  Order System","type":"projects"},{"authors":["Yaoran Chen","Yan Wang","Zhikun Dong","Jie Su","Zhaolong Han","Dai Zhou","Yongsheng Zhao","Yan Bao"],"categories":null,"content":"Abstract Short-term wind speed forecast is of great importance to wind farm regulation and its early warning. Previous studies mainly focused on the prediction at a single location but few extended the task to 2-D wind plane. In this study, a novel deep learning model was proposed for a 2-D regional wind speed forecast, using the combination of the auto-encoder of convolutional neural network (CNN) and the long short-term memory unit (LSTM). The 12-hidden-layer deep CNN was adopted to encode the high dimensional 2-D input into the embedding vector and inversely, to decode such latent representation after it was predicted by the LSTM module based on historical data. The model performance was compared with parallel models under different criteria, including MAE, RMSE and R2, all showing stable and considerable enhancements. For instance, the overall MAE value dropped to 0.35 m/s for the current model, which is 32.7%, 28.8% and 18.9% away from the prediction results using the persistence, basic ANN and LSTM model. Moreover, comprehensive discussions were provided from both temporal and spatial views of analysis, revealing that the current model can not only offer an accurate wind speed forecast along timeline (R2 equals to 0.981), but also give a distinct estimation of the spatial wind speed distribution in 2-D wind farm.\n","date":1621555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621555200,"objectID":"09b3aaa0423e581beb8831d91ae6a44c","permalink":"https://0324wy.github.io/publication/2d/","publishdate":"2021-05-21T00:00:00Z","relpermalink":"/publication/2d/","section":"publication","summary":"Energy\tConversion and Management. (IF 9.7)","tags":[],"title":"2-D regional short-term wind speed forecast based on CNN-LSTM deep learning model.","type":"publication"},{"authors":null,"categories":null,"content":"Short-term wind speed forecast is of great importance to wind farm regulation and its early warning. Previous studies mainly focused on the prediction at a single location but few extended the task to 2-D wind plane.\nIn this study, a novel deep learning model was proposed for a 2-D regional wind speed forecast, using the combination of the auto-encoder of convolutional neural network (CNN) and the long short-term memory unit (LSTM). The 12-hidden-layer deep CNN was adopted to encode the high dimensional 2-D input into the embedding vector and inversely, to decode such latent representation after it was predicted by the LSTM module based on historical data.\n","date":1609545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609545600,"objectID":"b29c2d5e8bf5b24f12dffabd4e088a8b","permalink":"https://0324wy.github.io/projects/windspeed/","publishdate":"2021-01-02T00:00:00Z","relpermalink":"/projects/windspeed/","section":"projects","summary":"Proposed a novel model called CNN-LSTM to enhance the accuracy of regional wind speed prediction. Drove the entire project from scratch, including selecting the research topic, finding open source data, thinking of ideas, and writing project code.","tags":["Machine Learning Appilication"],"title":"Deep Learning Model for Wind Speed Prediction","type":"projects"},{"authors":["Yaoran Chen","Zhikun Dong","Yan Wang","Jie Su","Zhaolong Han","Dai Zhou","Kai Zhang","Yongsheng Zhao","Yan Bao"],"categories":null,"content":"Abstract Accurate short-term wind speed prediction is of great significance for early warning and regulation of wind farms. At present, the scale of wind speed time-history data is increasing, and its time resolution is also becoming higher. Traditional machine learning models cannot effectively capture and utilize nonlinear features from the large scaled dataset and this, not only increases the difficulty of model building, but also reduces the prediction accuracy. To overcome such challenges, a machine learning based framework involving data-mining method was proposed in this paper. To begin with, a powerful signal decomposition technique (ensemble empirical mode decomposition) was used to divide the original wind sequence into several intrinsic mode functions to form a potential feature set. Then, a more appropriate sub-feature set together with the corresponding machine learning model were automatically generated through an iteration process. Such process was constructed through a coupled algorithm using the binary coded searching method known as the genetic algorithm and the advanced recurrent neural network with long short term memory unit. The analytical results show that, when compared with the traditional mainstream models, the strategy of using the sequences provided by the signal decomposition technology as the input features can significantly improve the prediction accuracy. On the other hand, faced with the high-dimensional feature set generated from the big data, the selected sub-feature set can not only perform a large dimension reduction, but also further improve the prediction accuracy up to 28.33% in terms of different kinds of evaluation criteria. Therefore, there is a potential application of the proposed method on more accurate short-term wind speed prediction under a considerable dataset of wind history.\n","date":1592697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592697600,"objectID":"571a46f833c9181ffbb7896143313ace","permalink":"https://0324wy.github.io/publication/short/","publishdate":"2020-06-21T00:00:00Z","relpermalink":"/publication/short/","section":"publication","summary":"Energy\tConversion and Management. (IF 9.7)","tags":[],"title":"Short-term wind speed predicting framework based on EEMD-GA-LSTM method under large scaled wind history.","type":"publication"},{"authors":["Yaoran Chen","Zhikun Dong","Jie Su","Yan Wang","Zhaolong Han","Dai Zhou","Yan Bao"],"categories":null,"content":"Abstract The maximum lift-to-drag coefficient of an airfoil directly affects the aerodynamic performance of wind turbine. Machine learning methods are known for being really effective in helping to predict this parameter in a faster and more accurate way. So far, the majority of related studies have focused on the use of artificial neural networks to make this prediction, but this model has issues with its poor interpretation and the confidence level of its results was unclear. In this paper, a novel framework is proposed, involving the Gaussian process regression and a hybrid feature mining process. The aim is to use the new framework to evaluate the maximum lift-to-drag ratio of given airfoils under a turbulent flow condition, where the Reynolds number is around 100,000. The feature mining process here designed contains a hybrid feature pool that comprises various geometric characters, and a hybrid feature selector that can assist the prediction performance and make it better. Based on the airfoil dataset of the University of Illinois at Urbana-Champaign that contains a total of 1432 profiles, a comparative analysis was conducted. The results showed that the current framework can provide a more accurate estimate than parallel models in both single-point and interval aspects of view. Noticeably, the model reached an overall precision of 95.2% and 94.1% on training and testing sets, respectively. Moreover, the simplicity and the confidence reference from the model output were further illustrated with a case study, which also verified that how it can serve real engineering application.\n","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582243200,"objectID":"2d6de4f8a6647b3ab3bb4ecd5a5b0a7b","permalink":"https://0324wy.github.io/publication/frame/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/publication/frame/","section":"publication","summary":"Energy\tConversion and Management. (IF 9.7)","tags":[],"title":"Framework of airfoil max lift-to-drag ratio prediction using hybrid feature mining and Gaussian process regression.","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://0324wy.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"}]