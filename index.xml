<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yan Wang</title>
    <link>https://0324wy.github.io/</link>
      <atom:link href="https://0324wy.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Yan Wang</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://0324wy.github.io/media/icon_hua5a76ffd5cf5a7c3caab46aff2309b2f_3105_512x512_fill_lanczos_center_3.png</url>
      <title>Yan Wang</title>
      <link>https://0324wy.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://0324wy.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TensorIR, RelayIR, and IRModule in TVM</title>
      <link>https://0324wy.github.io/blog/blog-4-tvmconcept/</link>
      <pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/blog/blog-4-tvmconcept/</guid>
      <description>&lt;h2 id=&#34;concept-of-tensorir-relayir-irmodule-prim_fun-tvm-script&#34;&gt;Concept of TensorIR, RelayIR, IRModule, Prim_fun, TVM Script&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Two levels represent the entire neural network program; the first level is a computational graph, and the second level is the tensor program. The previous one is called RelayIR, and the second one is called TensorIR. What we use in TVM to represent the TensorIR is called TVM Script.&lt;/li&gt;
&lt;li&gt;The tensor program contains the input and output, nested loop, and computation statement, which is a more fine-grained representation of the entire neural network program.&lt;/li&gt;
&lt;li&gt;The difference between the neural network program and the tensor program is the latter is framework-independent. It can represent the neural network program from PyTorch, TensorFlow, and others in a unified format.&lt;/li&gt;
&lt;li&gt;The above demonstration is from the vertical perspective.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://pic1.zhimg.com/70/v2-dcb979b6ad713fb751c11e1fb58b779b_1440w.image?source=172ae18b&amp;amp;biz_tag=Post&#34; alt=&#34;TVM 自底向上（一）：基本框架和概念&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;From the horizontal perspective, each graph contains multiple computation nodes(such as add, mm, ReLu, etc). Each of these nodes has its corresponding part in TensorIR; Each node corresponds to a prim_fun in TensorIR.&lt;/li&gt;
&lt;li&gt;Part of the graph contains one or more nodes, which is called a subgraph. Each subgraph corresponds to the combination of several prim_fun in TensorIR. The subgraph in RelayIR and the combination of several prim_fun in TensorIR are both called IRModule. Which is a container that includes several nodes or prim_func.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;four-ways-to-get-tensorir&#34;&gt;Four ways to get TensorIR&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Write TVM Script&lt;/li&gt;
&lt;li&gt;TE Expression&lt;/li&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;li&gt;Load from PyTorch&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;schedule-compute-decomposition&#34;&gt;Schedule Compute Decomposition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;source-code&#34;&gt;Source Code&lt;/h2&gt;
&lt;h3 id=&#34;object&#34;&gt;Object&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;All the IR datatypes should be inherent in the base class called Object, which defines the attributes of this class, and a function called VisitAttrs.&lt;/li&gt;
&lt;li&gt;In this way, the IR datatypes are in a unified format.&lt;/li&gt;
&lt;li&gt;Serialize/format/reflection/python bind/hash, to put it simply, interact with these IRs.&lt;/li&gt;
&lt;li&gt;The visitor pattern is a design pattern that makes the data and operations of that data separate. The benefits are from two aspects: 1. When we add new operations or change the original operations, we don&amp;rsquo;t have to change the code of data. 2. VisitAttrs allows us can process all the fields in a unified way.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;TensorNode&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Object&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;Array&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Expr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;content&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tensor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;Type&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;source&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;operation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;can&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;be&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;Operation&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;op&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;source&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;operation&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;/*&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt; \&lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;constructor&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;TensorNode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;VisitAttrs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;AttrVisitor&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;final&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Visit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;shape&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Visit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;dtype&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dtype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Visit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;op&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;op&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Visit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;value_index&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value_index&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;runtime&#34;&gt;Runtime&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is runtime? After we compile the code or the model code, we still need some other support to run the code, such as memory management, handling errors, etc. So, we develop a program to do that, and that program is called runtime.&lt;/li&gt;
&lt;li&gt;In the following example, you need to: 1. load the compiled model code; 2. initialize new data; 3. search the needed function; 4. run the function and print the result.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tvm&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mod&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tvm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;runtime&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load_module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;compiled_artifact.so&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# tvm.runtime.Module&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;arr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tvm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;device&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tvm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cuda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# tvm.runtime.NDArray&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;fun&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mod&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;addone&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# tvm.runtime.PackedFunc&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;fun&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;numpy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;shared_ptr&#34;&gt;shared_ptr&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;include/tvm/runtime/object.h defines Object and ObjectRef as two types.&lt;/li&gt;
&lt;li&gt;The ObjectRef can be seen as the share_ptr&amp;lt;Object)&lt;/li&gt;
&lt;li&gt;The shared_ptr has three features
&lt;ul&gt;
&lt;li&gt;Automatic management of memory, specifically, automatically deletes the Object if it is released.&lt;/li&gt;
&lt;li&gt;One Object could have multiple shared_ptr&lt;/li&gt;
&lt;li&gt;Reference counter: if the count of references is 0, the object will be deleted.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#include &amp;lt;iostream&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#include &amp;lt;memory&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shared_ptr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_shared&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Creates&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shared_ptr&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;an&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shared_ptr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Now&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;share&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ownership&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;same&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Use count: &amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use_count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;share&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ownership&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;reset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p2&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;releases&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ownership&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Use count after reset: &amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;use_count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;std&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;::&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;only&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p1&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;owns&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;now&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;packedfunc&#34;&gt;PackedFunc&lt;/h3&gt;
&lt;p&gt;==TODO==&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TVM has a Foreign Function Interface mechanism, which allows any language to call a function written by any language.&lt;/li&gt;
&lt;li&gt;What should we do for the function? First of all, we need to erase the parameter type and return type of the function.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;lt;tvm/runtime/packed_func.h&amp;gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;MyAdd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;TVMArgs&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TVMRetValue&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// Access arguments with type conversion
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rv&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// Set the return value
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;CallPacked&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;PackedFunc&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;myadd&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PackedFunc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MyAdd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// Wrap MyAdd as a PackedFunc
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;  &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;myadd&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;// Call PackedFunc, returns 3
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;module&#34;&gt;Module&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The result of the compilation is Runtime.Module, which is a hashmap&amp;lt;functionname, PackedFunc&amp;gt;.&lt;/li&gt;
&lt;li&gt;When this hashmap is retrieved, the caller gets the function and sets up the function and then runs the function.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Memory and Computation Cost of GPT</title>
      <link>https://0324wy.github.io/blog/blog-3-gptcost/</link>
      <pubDate>Thu, 08 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/blog/blog-3-gptcost/</guid>
      <description>&lt;h2 id=&#34;memory&#34;&gt;Memory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Activation values: 50 times the parameter memory&lt;/li&gt;
&lt;li&gt;Optimizer state: 2 times the parameter memory&lt;/li&gt;
&lt;li&gt;Gradients: 1 times the parameter memory&lt;/li&gt;
&lt;li&gt;KV Cache: 0.5 times the memory&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;number-of-parameters&#34;&gt;Number of Parameters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Number of parameters: approximately (number)：$12h^2l$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;memory-usage&#34;&gt;Memory Usage&lt;/h2&gt;
&lt;p&gt;Independent of input data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Memory usage during training: Excluding activation values&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parameters: float16 + float32：$\Phi$
&lt;ul&gt;
&lt;li&gt;Gradients: float16 + float32：$\Phi$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parameters for AdamW: float32：$2\Phi$&lt;/li&gt;
&lt;li&gt;Total：$20\Phi Bytes$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory usage during inference: Excluding activation values&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Parameters: float16：$\Phi$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Total: $2\Phi Bytes$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activation-values&#34;&gt;Activation Values&lt;/h2&gt;
&lt;p&gt;Approximately: per unit (Bytes)：$(34bsh + 5bs^2a)l$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When b=1, approximately 0.8 times the parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kv-cache&#34;&gt;KV Cache&lt;/h2&gt;
&lt;p&gt;Approximately: per unit (number)：$2b(s + n)l$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approximately 0.5 times the parameter memory&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computation&#34;&gt;Computation&lt;/h2&gt;
&lt;h2 id=&#34;computational-complexity&#34;&gt;Computational Complexity&lt;/h2&gt;
&lt;p&gt;Approximately: per unit (count)：$24bsh^2l$&lt;/p&gt;
&lt;h2 id=&#34;relationship-between-computational-complexity-and-number-of-parameters&#34;&gt;Relationship between Computational Complexity and Number of Parameters&lt;/h2&gt;
&lt;p&gt;Approximately: 2 times the number of tokens * number of parameters&lt;/p&gt;
&lt;p&gt;$$
\frac{24bsh^2l} {12h^2l * bs} = 2
$$&lt;/p&gt;
&lt;h2 id=&#34;computation-time&#34;&gt;Computation Time&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Backward propagation is approximately twice the computation of forward propagation.&lt;/li&gt;
&lt;li&gt;Activation value recomputation is approximately the same as forward propagation.&lt;/li&gt;
&lt;li&gt;Total is 8 times.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\frac{8 * number of tokens * number of parameters} {number of GPUs * peak flops per GPU * GPU utilization}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPU utilization is approximately between 0.3 and 0.55.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Designed and Built a Deep Learning Library Called Needle</title>
      <link>https://0324wy.github.io/projects/needle/</link>
      <pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/projects/needle/</guid>
      <description>&lt;p style=&#34;text-align: justify;&#34;&gt;Completed an online course on deep learning systems offered by CMU in order to delve into the internals of PyTorch and TensorFlow, and understand how they function at a fundamental level.&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Designed and built a deep learning library called Needle, capable of efficient GPU-based operations, automatic differentiation of all implemented functions, and the necessary modules to support parameterized layers, loss functions, data loaders, and optimizers.&lt;/p&gt;
&lt;h2 id=&#34;project-0&#34;&gt;Project 0&lt;/h2&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Build a basic softmax regression algorithm, plus a simple two-layer neural network. Create these implementations both in native Python (using the numpy library), and (for softmax regression) in native C/C++.&lt;/p&gt;
&lt;p&gt;✅ A basic &lt;code&gt;add&lt;/code&gt; function&lt;/p&gt;
&lt;p&gt;✅ Loading MNIST data:  &lt;code&gt;parse_mnist&lt;/code&gt; function&lt;/p&gt;
&lt;p&gt;✅Softmax loss: &lt;code&gt;softmax_loss&lt;/code&gt; function&lt;/p&gt;
&lt;p&gt;✅Stochastic gradient descent for softmax regression&lt;/p&gt;
&lt;p&gt;✅SGD for a two-layer neural network&lt;/p&gt;
&lt;p&gt;✅Softmax regression in C++&lt;/p&gt;
&lt;h2 id=&#34;project-1&#34;&gt;Project 1&lt;/h2&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Build a basic &lt;strong&gt;automatic differentiation&lt;/strong&gt; framework, then use this to re-implement the simple two-layer neural network we used for the MNIST digit classification problem in HW0.&lt;/p&gt;
&lt;p&gt;✅Implementing forward computation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅PowerScalar&lt;/li&gt;
&lt;li&gt;✅EWiseDiv&lt;/li&gt;
&lt;li&gt;✅DivScalar&lt;/li&gt;
&lt;li&gt;✅MatMul&lt;/li&gt;
&lt;li&gt;✅Summation&lt;/li&gt;
&lt;li&gt;✅BroadcastTo&lt;/li&gt;
&lt;li&gt;✅Reshape&lt;/li&gt;
&lt;li&gt;✅Negate&lt;/li&gt;
&lt;li&gt;✅Transpose&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;✅Implementing backward computation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅EWiseDiv&lt;/li&gt;
&lt;li&gt;✅DivScalar&lt;/li&gt;
&lt;li&gt;✅MatMul&lt;/li&gt;
&lt;li&gt;✅Summation&lt;/li&gt;
&lt;li&gt;✅BroadcastTo&lt;/li&gt;
&lt;li&gt;✅Reshape&lt;/li&gt;
&lt;li&gt;✅Negate&lt;/li&gt;
&lt;li&gt;✅Transpose&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;✅Topological sort: allow us to traverse through (forward or backward) the computation graph, computing gradients along the way&lt;/p&gt;
&lt;p&gt;✅Implementing reverse mode differentiation&lt;/p&gt;
&lt;p&gt;✅Softmax loss&lt;/p&gt;
&lt;p&gt;✅SGD for a two-layer neural network&lt;/p&gt;
&lt;h2 id=&#34;project-2&#34;&gt;Project 2&lt;/h2&gt;
&lt;p&gt;Implement a &lt;strong&gt;neural network library&lt;/strong&gt; in the needle framework.&lt;/p&gt;
&lt;p&gt;✅Implement a few different methods for weight initialization&lt;/p&gt;
&lt;p&gt;✅Implement additional modules&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅Linear: &lt;code&gt;needle.nn.Linear&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅ReLU:&lt;code&gt;needle.nn.ReLU&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅Sequential: &lt;code&gt;needle.nn.Sequential&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅LogSumExp: &lt;code&gt;needle.ops.LogSumExp&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅SoftmaxLoss: &lt;code&gt;needle.nn.SoftmaxLoss&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅LayerNorm1d: &lt;code&gt;needle.nn.LayerNorm1d&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅Flatten: &lt;code&gt;needle.nn.Flatten&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅BatchNorm1d: &lt;code&gt;needle.nn.BatchNorm1d&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅Dropout: &lt;code&gt;needle.nn.Dropout&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅Residual: &lt;code&gt;needle.nn.Residual&lt;/code&gt; class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;✅Implement the &lt;code&gt;step&lt;/code&gt; function of the following optimizers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅SGD: &lt;code&gt;needle.optim.SGD&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅Adam: &lt;code&gt;needle.optim.Adam&lt;/code&gt; class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;✅Implement two data primitives: &lt;code&gt;needle.data.DataLoader&lt;/code&gt; and &lt;code&gt;needle.data.Dataset&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅Transformations: &lt;code&gt;RandomFlipHorizontal&lt;/code&gt; function and &lt;code&gt;RandomFlipHorizontal&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅Dataset: &lt;code&gt;needle.data.MNISTDataset&lt;/code&gt; class&lt;/li&gt;
&lt;li&gt;✅Dataloader: &lt;code&gt;needle.data.Dataloader&lt;/code&gt; class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;✅Build and train an MLP ResNet&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅ResidualBlock: &lt;code&gt;ResidualBlock&lt;/code&gt; function&lt;/li&gt;
&lt;li&gt;✅MLPResNet: &lt;code&gt;MLPResNet&lt;/code&gt; function&lt;/li&gt;
&lt;li&gt;✅Epoch: &lt;code&gt;epoch&lt;/code&gt; function&lt;/li&gt;
&lt;li&gt;✅Train Mnist: &lt;code&gt;train_mnist&lt;/code&gt; function&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;project-3&#34;&gt;Project 3&lt;/h2&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Build a simple backing library for the processing that underlies most deep learning systems: &lt;strong&gt;the n-dimensional array&lt;/strong&gt; (a.k.a. the NDArray).&lt;/p&gt;
&lt;p&gt;✅Python array operations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅reshape: &lt;code&gt;reshape&lt;/code&gt; function&lt;/li&gt;
&lt;li&gt;✅permute: &lt;code&gt;permute&lt;/code&gt; function&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Good Open Courses for Computer Science Learning</title>
      <link>https://0324wy.github.io/blog/blog-2-csmaterial/</link>
      <pubDate>Mon, 13 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/blog/blog-2-csmaterial/</guid>
      <description>&lt;h2 id=&#34;introduction-to-computer-science&#34;&gt;Introduction to Computer Science&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MIT 6.0001&lt;/li&gt;
&lt;li&gt;Harvard CS50&lt;/li&gt;
&lt;li&gt;Berkeley CS61A&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-structures-and-algorithms&#34;&gt;Data Structures and Algorithms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stanford CS106&lt;/li&gt;
&lt;li&gt;MIT 6.006, 6.046&lt;/li&gt;
&lt;li&gt;Berkeley CS61A, CS61B&lt;/li&gt;
&lt;li&gt;Princeton&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;operating-systems&#34;&gt;Operating Systems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CMU 15-213&lt;/li&gt;
&lt;li&gt;Berkeley CS162, CS262&lt;/li&gt;
&lt;li&gt;MIT 6.828&lt;/li&gt;
&lt;li&gt;6.S081&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computer-organization-and-architecture&#34;&gt;Computer Organization and Architecture&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MIT 6.004&lt;/li&gt;
&lt;li&gt;Berkeley CS61C&lt;/li&gt;
&lt;li&gt;MHRD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computer-networks&#34;&gt;Computer Networks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stanford CS144&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;databases&#34;&gt;Databases&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CMU 15-445&lt;/li&gt;
&lt;li&gt;MIT 6.824, 6.830&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;software-engineering&#34;&gt;Software Engineering&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MIT 6.031&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;distributed-systems&#34;&gt;Distributed Systems&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MIT 6.824&lt;/li&gt;
&lt;li&gt;MIT 6.033
&lt;ul&gt;
&lt;li&gt;Principles of Computer System Design: An Introduction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;compiler-design&#34;&gt;Compiler Design&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Stanford CS143&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;parallel-computing&#34;&gt;Parallel Computing&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CS 149&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computer-graphics&#34;&gt;Computer Graphics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Games101&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computer-system-security&#34;&gt;Computer System Security&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;6.858&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;projects&#34;&gt;Projects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Operating System: 6.828&lt;/li&gt;
&lt;li&gt;Distributed Systems: 6.824&lt;/li&gt;
&lt;li&gt;Relational Database: CMU 15-445&lt;/li&gt;
&lt;li&gt;Parallel Computing: CS 149&lt;/li&gt;
&lt;li&gt;Compiler Design: Stanford CS143&lt;/li&gt;
&lt;li&gt;Computer Graphics: Games101&lt;/li&gt;
&lt;li&gt;Computer Networks: Stanford CS144&lt;/li&gt;
&lt;li&gt;Computer System Security: 6.858&lt;/li&gt;
&lt;li&gt;CSAPP&lt;/li&gt;
&lt;li&gt;Muduo: Web Server&lt;/li&gt;
&lt;li&gt;TinSTL: STL&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Tiger Book&amp;rdquo;: Compiler&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CS DIY: &lt;a href=&#34;https://csdiy.wiki/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS DIY Wiki&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Experience of Self-learning Driven by Interest</title>
      <link>https://0324wy.github.io/blog/blog-1-experience/</link>
      <pubDate>Mon, 13 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/blog/blog-1-experience/</guid>
      <description>&lt;p style=&#34;text-align: justify;&#34;&gt;Unlike my peers majoring in computer science, I pursued a self-learning path based on my interests. While encountering some difficulties, the journey was also highly rewarding.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;My first exposure to computer science was during my freshman year, where I learned C language - my first programming language. I found it fascinating and read the entire textbook thoroughly. As a result, I performed well in the course and scored 95 points. In my sophomore and junior years, I participated in mathematical modeling competitions and was introduced to neural networks. Although I didn&amp;rsquo;t fully understand them at the time, it sparked my curiosity to learn more. In my senior year, I heard about concepts like deep learning and artificial intelligence, and realized that they were built on neural networks, which I had not yet fully grasped. Driven by my curiosity, I planned to systematically learn the principles of these topics.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Throughout my graduate studies, starting from my senior year, I continued to explore computer science knowledge driven by my passion. I delved into Python, Machine Learning, Deep Learning, Java, C++, Data Structures and Algorithms. Along the way, I was introduced to lower-level concepts, so I started learning Operating Systems, Computer Networks, Database Principles, and Design Patterns. I also pursued many mathematics courses, including Matrix Theory, Partial Differential Equation, and Computational Methods, with the aim of becoming a well-rounded engineering student with a strong foundation in mathematics and programming skills.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;During my master&amp;rsquo;s degree, I chose to research on the intersection of deep learning and civil engineering. In a situation where the research group had no accumulation in this area, I had to figure things out on my own. I aspired to apply for a Ph.D. in computer science during my graduate studies, but struggled to find a clear path. I eventually decided to work instead. However, as I gained more exposure to information, I discovered that a path to a Ph.D. could be pursued through internships at research labs to gain research experience and skills.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;After graduation, I worked as a web backend developer at Baidu. As the entire backend system was a distributed system, I began learning about distributed systems and studied various open-source project architectures, such as Redis and Zookeeper. I also read papers on machine learning, including the Bert and GPT series.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Looking back on my entire experience of self-learning, I&amp;rsquo;ve realized that I&amp;rsquo;ve gained not only knowledge, but also the ability to select appropriate learning materials, effectively manage study time and energy, and develop the courage to overcome difficulties. With these skills, I am excited to continue exploring new areas of knowledge and challenging myself further.&lt;br&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;My interest in both distributed systems and machine learning is the reason I want to work in MLSys. I believe that the infrastructure for large models is critical and promising. Additionally, this field is closer to industry and practical applications compared to other fields. The results of this field are more easily deployed in the development process and products, rather than just remaining on paper. This aligns with my research goals, which aim to have a positive impact on subsequent research in this field and its applications in industry.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Baidu E-commerce Distributed  Order System</title>
      <link>https://0324wy.github.io/projects/baidu_e_commerce/</link>
      <pubDate>Fri, 27 Jan 2023 10:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/projects/baidu_e_commerce/</guid>
      <description>&lt;h1 id=&#34;baidu-e-commerce-middle-platform&#34;&gt;Baidu E-commerce Middle Platform&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;This project is the Baidu E-commerce Middle Platform project. The role of the E-commerce Middle Platform is to provide common e-commerce capabilities for Front Platform business parties, such as ordering, after-sales, promotions, and product management.&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;It is divided into four small teams, including Shop Center, Commodity Center, Marketing Center, and Trading Center. The Shop Center is mainly used to manage the stores of B-side merchants. The Commodity Center is used to manage the inventory and qualifications of products. The Trading Center is used to provide services such as ordering, shopping cart ordering, after-sales, and logistics inquiry. The Marketing Center is used to provide services such as creating coupons for merchants and calculating marketing prices.&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Front Platform business parties include products from different industries such as education, e-commerce, medical beauty, and life services. For example, AiQiCha is a product for the enterprise service industry, and ZhiLiaoHaoXue is a product for the education industry. Users enter the corresponding Front Platform products by clicking related links through the Baidu search engine. When users have e-commerce-related needs, they will use the services of the Middle Platform. For example, using the education industry product ZhiLiaoHaoXue to purchase an online course.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://p.ipic.vip/xmahh5.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;design-and-architecture&#34;&gt;Design and Architecture&lt;/h2&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Taking the example of the Front Platform application DuXiaoDian calling the Trading Center, there are a total of seven modules in the entire chain, each of which is independently deployed and uses a cluster architecture. The CDN servers, reverse proxy servers, and load balancing servers sit at the top of the chain. The Trading Center connects to both distributed database servers and distributed cache servers.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://p.ipic.vip/z5bxkm.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CDN Servers&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;CDN servers are designed to improve the availability and performance of a service by distributing it geographically closer to end-users. Essentially, a CDN is a cache server located closer to the users to speed up their access.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reverse Proxy Servers&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Reverse proxy servers serve a similar function to CDN servers, but they are located on the side of the website&amp;rsquo;s data center. Additionally, they are responsible for ensuring security and acting as a barrier between the web server and external traffic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loading Balancing Servers&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Load balancing servers are positioned in front of your backend servers and are responsible for distributing client requests across a group of servers. This is done in a way that maximizes speed and capacity utilization, while also ensuring that no single server becomes overloaded and negatively impacts performance. In the event that a server goes down, the load balancer redirects traffic to the remaining online servers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distributed Cache Servers&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Distributed cache servers are servers that store webpages or other internet content locally. By storing frequently accessed data in temporary storage, cache servers both speed up access to data and reduce the demand on an enterprise&amp;rsquo;s bandwidth.&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;In this project, we are using Redis cluster as our distributed cache servers. Redis is known for its high-performance data read and write functions, and is widely used in caching scenarios. By using Redis, we can improve the performance of our business systems and better withstand high levels of concurrent traffic requests to the database.&lt;/p&gt;
&lt;h2 id=&#34;core-question&#34;&gt;Core Question&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;How To Solve the Data Consistency Issues Between Redis And MySQL?&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;This is also a classic problem in distributed systems - how to ensure data consistency between two heterogeneous systems.&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Setting up Redis cache is to improve the performance of the system by copying hot data from MySQL to Redis. When a read request is received, it is directly read from Redis instead of from MySQL database. If the data is not available in the Redis cache, it is read from the database, and the data is then copied from the database to Redis.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://p.ipic.vip/pe646f.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;However, this can cause a problem of data inconsistency between the two systems. For example, if the data X = 1 in MySQL is modified to X = 2, the old data X = 1 will still exist in the Redis cache.&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;If strong consistency is guaranteed, availability will be sacrificed. When modifying MySQL data, Redis must be updated synchronously, and no other read requests can be accepted during the update.&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;To ensure availability, only eventual consistency can be guaranteed. In this project, a solution to ensure eventual consistency is adopted. When the data X = 1 in MySQL is modified to X = 2, MySQL generates a Binlog record of the data change, and when the Binlog is listened to, the change record is added to the message queue. The consumer side consumes this message by deleting the corresponding data from Redis. When a new request comes in, if the data is not in the Redis cache, it is read from the database and copied to Redis. This ensures eventual consistency between Redis and MySQL data.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://p.ipic.vip/ixrc06.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Deleting the data X = 1 from Redis instead of modifying it to X = 2 is because the cached data may not be read immediately when data changes occur, so deleting the cache improves its utilization.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2-D regional short-term wind speed forecast based on CNN-LSTM deep learning model.</title>
      <link>https://0324wy.github.io/publication/2d/</link>
      <pubDate>Fri, 21 May 2021 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/publication/2d/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Short-term wind speed forecast is of great importance to wind farm regulation and its early warning. Previous studies mainly focused on the prediction at a single location but few extended the task to 2-D wind plane. In this study, a novel deep learning model was proposed for a 2-D regional wind speed forecast, using the combination of the auto-encoder of convolutional neural network (CNN) and the long short-term memory unit (LSTM). The 12-hidden-layer deep CNN was adopted to encode the high dimensional 2-D input into the embedding vector and inversely, to decode such latent representation after it was predicted by the LSTM module based on historical data. The model performance was compared with parallel models under different criteria, including MAE, RMSE and R2, all showing stable and considerable enhancements. For instance, the overall MAE value dropped to 0.35 m/s for the current model, which is 32.7%, 28.8% and 18.9% away from the prediction results using the persistence, basic ANN and LSTM model. Moreover, comprehensive discussions were provided from both temporal and spatial views of analysis, revealing that the current model can not only offer an accurate wind speed forecast along timeline (R2 equals to 0.981), but also give a distinct estimation of the spatial wind speed distribution in 2-D wind farm.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Model for Wind Speed Prediction</title>
      <link>https://0324wy.github.io/projects/windspeed/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/projects/windspeed/</guid>
      <description>&lt;p style=&#34;text-align: justify;&#34;&gt;Short-term wind speed forecast is of great importance to wind farm regulation and its early warning. Previous studies mainly focused on the prediction at a single location but few extended the task to 2-D wind plane.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://p.ipic.vip/7xoehl.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;In this study, a novel deep learning model was proposed for a 2-D regional wind speed forecast, using the combination of the auto-encoder of convolutional neural network (CNN) and the long short-term memory unit (LSTM). The 12-hidden-layer deep CNN was adopted to encode the high dimensional 2-D input into the embedding vector and inversely, to decode such latent representation after it was predicted by the LSTM module based on historical data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Short-term wind speed predicting framework based on EEMD-GA-LSTM method under large scaled wind history.</title>
      <link>https://0324wy.github.io/publication/short/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/publication/short/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;Accurate short-term wind speed prediction is of great significance for early warning and regulation of wind farms. At present, the scale of wind speed time-history data is increasing, and its time resolution is also becoming higher. Traditional machine learning models cannot effectively capture and utilize nonlinear features from the large scaled dataset and this, not only increases the difficulty of model building, but also reduces the prediction accuracy. To overcome such challenges, a machine learning based framework involving data-mining method was proposed in this paper. To begin with, a powerful signal decomposition technique (ensemble empirical mode decomposition) was used to divide the original wind sequence into several intrinsic mode functions to form a potential feature set. Then, a more appropriate sub-feature set together with the corresponding machine learning model were automatically generated through an iteration process. Such process was constructed through a coupled algorithm using the binary coded searching method known as the genetic algorithm and the advanced recurrent neural network with long short term memory unit. The analytical results show that, when compared with the traditional mainstream models, the strategy of using the sequences provided by the signal decomposition technology as the input features can significantly improve the prediction accuracy. On the other hand, faced with the high-dimensional feature set generated from the big data, the selected sub-feature set can not only perform a large dimension reduction, but also further improve the prediction accuracy up to 28.33% in terms of different kinds of evaluation criteria. Therefore, there is a potential application of the proposed method on more accurate short-term wind speed prediction under a considerable dataset of wind history.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Framework of airfoil max lift-to-drag ratio prediction using hybrid feature mining and Gaussian process regression.</title>
      <link>https://0324wy.github.io/publication/frame/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/publication/frame/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p style=&#34;text-align: justify;&#34;&gt;The maximum lift-to-drag coefficient of an airfoil directly affects the aerodynamic performance of wind turbine. Machine learning methods are known for being really effective in helping to predict this parameter in a faster and more accurate way. So far, the majority of related studies have focused on the use of artificial neural networks to make this prediction, but this model has issues with its poor interpretation and the confidence level of its results was unclear. In this paper, a novel framework is proposed, involving the Gaussian process regression and a hybrid feature mining process. The aim is to use the new framework to evaluate the maximum lift-to-drag ratio of given airfoils under a turbulent flow condition, where the Reynolds number is around 100,000. The feature mining process here designed contains a hybrid feature pool that comprises various geometric characters, and a hybrid feature selector that can assist the prediction performance and make it better. Based on the airfoil dataset of the University of Illinois at Urbana-Champaign that contains a total of 1432 profiles, a comparative analysis was conducted. The results showed that the current framework can provide a more accurate estimate than parallel models in both single-point and interval aspects of view. Noticeably, the model reached an overall precision of 95.2% and 94.1% on training and testing sets, respectively. Moreover, the simplicity and the confidence reference from the model output were further illustrated with a case study, which also verified that how it can serve real engineering application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Project</title>
      <link>https://0324wy.github.io/project/example/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/project/example/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://0324wy.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://0324wy.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
